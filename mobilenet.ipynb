{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10298344,"sourceType":"datasetVersion","datasetId":6374214}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torchvision import transforms, models, datasets\nimport torch.optim as optim\nimport torch.nn as nn\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, precision_recall_fscore_support)\nfrom transformers import ViTForImageClassification, ViTFeatureExtractor\nimport optuna","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-25T20:40:05.526610Z","iopub.execute_input":"2024-12-25T20:40:05.527022Z","iopub.status.idle":"2024-12-25T20:40:21.833902Z","shell.execute_reply.started":"2024-12-25T20:40:05.526987Z","shell.execute_reply":"2024-12-25T20:40:21.833212Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"dataset_dir = '/kaggle/input/deepfake/DeepFake'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T20:40:21.840023Z","iopub.execute_input":"2024-12-25T20:40:21.840309Z","iopub.status.idle":"2024-12-25T20:40:21.857208Z","shell.execute_reply.started":"2024-12-25T20:40:21.840278Z","shell.execute_reply":"2024-12-25T20:40:21.856579Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"transform_train = transforms.Compose([\n    transforms.Resize((224, 224)),  \n    transforms.ToTensor(),  \n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(p=0.2),\n    transforms.RandomRotation(15),\n    transforms.RandomCrop(224, padding=10),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.RandomAffine(degrees=20, scale=(0.8, 1.2), shear=10),\n    transforms.RandomErasing(p=0.3),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  \n])\n\ntransform_val_test = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T20:40:21.857960Z","iopub.execute_input":"2024-12-25T20:40:21.858169Z","iopub.status.idle":"2024-12-25T20:40:21.872987Z","shell.execute_reply.started":"2024-12-25T20:40:21.858150Z","shell.execute_reply":"2024-12-25T20:40:21.872174Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Load the dataset\nfrom torchvision.datasets import ImageFolder\ndataset = ImageFolder(root=dataset_dir, transform=transform_train)\nprint(\"Classes:\", dataset.classes)\nprint(\"Class-to-Index Mapping:\", dataset.class_to_idx)\nprint(\"Number of Samples:\", len(dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T20:40:21.873865Z","iopub.execute_input":"2024-12-25T20:40:21.874102Z","iopub.status.idle":"2024-12-25T20:40:28.440707Z","shell.execute_reply.started":"2024-12-25T20:40:21.874072Z","shell.execute_reply":"2024-12-25T20:40:28.439941Z"}},"outputs":[{"name":"stdout","text":"Classes: ['Fake', 'Real']\nClass-to-Index Mapping: {'Fake': 0, 'Real': 1}\nNumber of Samples: 10826\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"def get_model(model_name):\n    if model_name == \"mobilenet-v2\":\n        # Load pre-trained MobileNet-v2 model\n        model = models.mobilenet_v2(pretrained=True)\n        \n        # Freeze all layers initially\n        for param in model.parameters():\n            param.requires_grad = False\n        \n        # Update the classifier for binary classification\n        model.classifier[1] = nn.Linear(model.last_channel, 2)\n        \n        return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T20:41:55.350245Z","iopub.execute_input":"2024-12-25T20:41:55.350552Z","iopub.status.idle":"2024-12-25T20:41:55.355258Z","shell.execute_reply.started":"2024-12-25T20:41:55.350530Z","shell.execute_reply":"2024-12-25T20:41:55.354314Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Calculate metrics function\ndef calculate_metrics(model, loader, device):\n    \n    # Set the model to evaluation mode (disables dropout)\n    model.eval()\n\n    # Lists to store true labels and predicted labels\n    all_labels = []\n    all_predictions = []\n\n    # Disabling gradient computation\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n             # Get predicted labels by taking the argmax (most likely class)\n            _, predicted = torch.max(outputs, 1)\n            all_labels.extend(labels.cpu().numpy())\n            all_predictions.extend(predicted.cpu().numpy())\n\n     # Calculate the confusion matrix,which give TN, FP, FN, and TP\n    conf_matrix = confusion_matrix(all_labels, all_predictions)\n    # Unpack the confusion matrix into four components: TN, FP, FN, TP\n    TN, FP, FN, TP = conf_matrix.ravel() \n\n    total = conf_matrix.sum()\n    accuracy = (TP + TN) / total if total > 0 else 0.0\n    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n    recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n    \n    return accuracy, precision, recall, f1, conf_matrix\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T20:43:39.789975Z","iopub.execute_input":"2024-12-25T20:43:39.790340Z","iopub.status.idle":"2024-12-25T20:43:39.796766Z","shell.execute_reply.started":"2024-12-25T20:43:39.790313Z","shell.execute_reply":"2024-12-25T20:43:39.795779Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Train the model function with validation accuracy printed after each epoch\ndef train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs=5):\n    # Variable to track the best validation accuracy\n    best_val_accuracy = 0\n    \n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n         # Iterate over batches in the training data\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n        \n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n        # Validation phase\n        model.eval()\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                _, predicted = torch.max(outputs, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n        \n        val_accuracy = 100 * correct / total\n        print(f\"Epoch {epoch+1}/{epochs}, Validation Accuracy: {val_accuracy:.2f}%\")\n        \n        if val_accuracy > best_val_accuracy:\n            best_val_accuracy = val_accuracy\n    \n    return best_val_accuracy\n\n# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Cross-validation setup\nnum_folds = 3\nkf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T20:44:06.550175Z","iopub.execute_input":"2024-12-25T20:44:06.550520Z","iopub.status.idle":"2024-12-25T20:44:06.639566Z","shell.execute_reply.started":"2024-12-25T20:44:06.550493Z","shell.execute_reply":"2024-12-25T20:44:06.638515Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def objective(trial, model_name):\n    # Get a suggested learning rate from Optuna\n    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n    \n    # Initialize the model with dropout\n    model = get_model(model_name).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n    val_accuracies = []\n    for fold_idx, (train_val_idx, test_idx) in enumerate(kf.split(dataset)):\n        print(f\"Fold {fold_idx + 1}/{num_folds}\")\n        \n        # Create training/validation split\n        train_val_data = Subset(dataset, train_val_idx)\n        test_data = Subset(dataset, test_idx)\n        \n        train_size = int(0.8 * len(train_val_data))\n        val_size = len(train_val_data) - train_size\n        train_data, val_data = torch.utils.data.random_split(\n            train_val_data, [train_size, val_size], generator=torch.Generator().manual_seed(42)\n        )\n        train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n        val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n        \n        # Train the model and get validation accuracy\n        train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs=5)\n        \n        # Evaluate on validation set\n        val_accuracy, _, _, _, _ = calculate_metrics(model, val_loader, device)\n        val_accuracies.append(val_accuracy)\n    \n    # Return the average validation accuracy across all folds as the objective value\n    return np.mean(val_accuracies)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T20:44:23.147807Z","iopub.execute_input":"2024-12-25T20:44:23.148081Z","iopub.status.idle":"2024-12-25T20:44:23.154159Z","shell.execute_reply.started":"2024-12-25T20:44:23.148061Z","shell.execute_reply":"2024-12-25T20:44:23.153454Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def evaluate_test_set(model_name, best_lr):\n    # Initialize model with the best learning rate\n    model = get_model(model_name).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=best_lr)\n    criterion = nn.CrossEntropyLoss()\n\n    fold_metrics = []\n    for fold_idx, (train_val_idx, test_idx) in enumerate(kf.split(dataset)):\n        print(f\"\\nEvaluating on Fold {fold_idx + 1}/{num_folds}\")\n        \n        # Create training/validation split\n        train_val_data = Subset(dataset, train_val_idx)\n        test_data = Subset(dataset, test_idx)\n        \n        train_size = int(0.8 * len(train_val_data))\n        val_size = len(train_val_data) - train_size\n        train_data, val_data = torch.utils.data.random_split(\n            train_val_data, [train_size, val_size], generator=torch.Generator().manual_seed(42)\n        )\n        train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n        val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n        \n        # Train the model\n        train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs=5)\n        \n        # Evaluate on the test set\n        test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n        fold_metrics.append(calculate_metrics(model, test_loader, device))\n    \n    # Print metrics for each fold\n    for fold_idx, metrics in enumerate(fold_metrics):\n        accuracy, precision, recall, f1, conf_matrix = metrics\n        print(f\"Fold {fold_idx + 1} Metrics:\")\n        print(f\"Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}\")\n        print(f\"Confusion Matrix:\\n{conf_matrix}\")\n\n# Calculate average metrics across folds\n    avg_accuracy = np.mean([metrics[0] for metrics in fold_metrics])\n    avg_precision = np.mean([metrics[1] for metrics in fold_metrics])\n    avg_recall = np.mean([metrics[2] for metrics in fold_metrics])\n    avg_f1 = np.mean([metrics[3] for metrics in fold_metrics])\n    total_conf_matrix = np.sum([metrics[4] for metrics in fold_metrics], axis=0)\n\n    print(\"\\nAverage Metrics Across Folds:\")\n    print(f\"Accuracy: {avg_accuracy:.2f}, Precision: {avg_precision:.2f}, Recall: {avg_recall:.2f}, F1-Score: {avg_f1:.2f}\")\n    print(f\"Confusion Matrix (sum of all folds):\\n{total_conf_matrix}\")\n\n\n# Optuna Optimization and Final Testing\nfor model_name in [\"mobilenet-v2\"]:\n    print(f\"\\nOptimizing for {model_name.upper()}...\")\n    study = optuna.create_study(direction='maximize')\n    study.optimize(lambda trial: objective(trial, model_name), n_trials=5)  # You can increase the number of trials if needed\n\n    # Best learning rate found for the model\n    best_lr = study.best_params['lr']\n    print(f\"Best Learning Rate for {model_name.upper()}: {best_lr}\")\n\n    # Evaluate on test sets for each fold\n    evaluate_test_set(model_name, best_lr)\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T20:44:59.559090Z","iopub.execute_input":"2024-12-25T20:44:59.559379Z","iopub.status.idle":"2024-12-25T23:15:39.189253Z","shell.execute_reply.started":"2024-12-25T20:44:59.559340Z","shell.execute_reply":"2024-12-25T23:15:39.188436Z"}},"outputs":[{"name":"stderr","text":"[I 2024-12-25 20:44:59,565] A new study created in memory with name: no-name-ca8333eb-a895-4121-bf86-3b81196e7293\n","output_type":"stream"},{"name":"stdout","text":"\nOptimizing for MOBILENET-V2...\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-11-2e749100ab16>:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n100%|██████████| 13.6M/13.6M [00:00<00:00, 111MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Fold 1/3\nEpoch 1/5, Loss: 0.7147341576399724\nEpoch 1/5, Validation Accuracy: 64.96%\nEpoch 2/5, Loss: 0.7273419736498627\nEpoch 2/5, Validation Accuracy: 66.48%\nEpoch 3/5, Loss: 0.6947253484752297\nEpoch 3/5, Validation Accuracy: 67.59%\nEpoch 4/5, Loss: 0.7314096708982689\nEpoch 4/5, Validation Accuracy: 64.68%\nEpoch 5/5, Loss: 0.7058119304601659\nEpoch 5/5, Validation Accuracy: 70.78%\nFold 2/3\nEpoch 1/5, Loss: 0.7092787017479786\nEpoch 1/5, Validation Accuracy: 65.44%\nEpoch 2/5, Loss: 0.6859393575902801\nEpoch 2/5, Validation Accuracy: 63.71%\nEpoch 3/5, Loss: 0.7197405669570628\nEpoch 3/5, Validation Accuracy: 68.14%\nEpoch 4/5, Loss: 0.6931390834776736\nEpoch 4/5, Validation Accuracy: 68.14%\nEpoch 5/5, Loss: 0.6936759021730055\nEpoch 5/5, Validation Accuracy: 69.11%\nFold 3/3\nEpoch 1/5, Loss: 0.7226736082885805\nEpoch 1/5, Validation Accuracy: 69.74%\nEpoch 2/5, Loss: 0.7062867117520853\nEpoch 2/5, Validation Accuracy: 67.24%\nEpoch 3/5, Loss: 0.7055264796999937\nEpoch 3/5, Validation Accuracy: 69.25%\nEpoch 4/5, Loss: 0.7347591591803408\nEpoch 4/5, Validation Accuracy: 69.11%\nEpoch 5/5, Loss: 0.7647223293122666\nEpoch 5/5, Validation Accuracy: 60.46%\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-25 21:10:11,397] Trial 0 finished with value: 0.6696675900277009 and parameters: {'lr': 0.004443927960100261}. Best is trial 0 with value: 0.6696675900277009.\n<ipython-input-11-2e749100ab16>:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Fold 1/3\nEpoch 1/5, Loss: 0.818146201787074\nEpoch 1/5, Validation Accuracy: 65.58%\nEpoch 2/5, Loss: 0.7048153915128655\nEpoch 2/5, Validation Accuracy: 63.43%\nEpoch 3/5, Loss: 0.7258577221664935\nEpoch 3/5, Validation Accuracy: 69.25%\nEpoch 4/5, Loss: 0.7490042462862657\nEpoch 4/5, Validation Accuracy: 65.44%\nEpoch 5/5, Loss: 0.8220236660367217\nEpoch 5/5, Validation Accuracy: 65.17%\nFold 2/3\nEpoch 1/5, Loss: 0.7808067306299895\nEpoch 1/5, Validation Accuracy: 67.04%\nEpoch 2/5, Loss: 0.7537847120129602\nEpoch 2/5, Validation Accuracy: 58.86%\nEpoch 3/5, Loss: 0.7896632203081037\nEpoch 3/5, Validation Accuracy: 70.50%\nEpoch 4/5, Loss: 0.7315936771874928\nEpoch 4/5, Validation Accuracy: 63.92%\nEpoch 5/5, Loss: 0.7528217691742913\nEpoch 5/5, Validation Accuracy: 68.98%\nFold 3/3\nEpoch 1/5, Loss: 0.7746059664046567\nEpoch 1/5, Validation Accuracy: 69.39%\nEpoch 2/5, Loss: 0.7469685533758026\nEpoch 2/5, Validation Accuracy: 69.53%\nEpoch 3/5, Loss: 0.7855175985486468\nEpoch 3/5, Validation Accuracy: 69.32%\nEpoch 4/5, Loss: 0.7378212356764967\nEpoch 4/5, Validation Accuracy: 58.24%\nEpoch 5/5, Loss: 0.7708779912627204\nEpoch 5/5, Validation Accuracy: 62.40%\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-25 21:35:14,182] Trial 1 finished with value: 0.6491228070175439 and parameters: {'lr': 0.006235132000782023}. Best is trial 0 with value: 0.6696675900277009.\n<ipython-input-11-2e749100ab16>:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Fold 1/3\nEpoch 1/5, Loss: 0.7216680925854003\nEpoch 1/5, Validation Accuracy: 53.19%\nEpoch 2/5, Loss: 0.7022787181053373\nEpoch 2/5, Validation Accuracy: 53.05%\nEpoch 3/5, Loss: 0.6875540077357002\nEpoch 3/5, Validation Accuracy: 58.17%\nEpoch 4/5, Loss: 0.6789502254507159\nEpoch 4/5, Validation Accuracy: 62.12%\nEpoch 5/5, Loss: 0.6663403718511044\nEpoch 5/5, Validation Accuracy: 61.84%\nFold 2/3\nEpoch 1/5, Loss: 0.6644435594095051\nEpoch 1/5, Validation Accuracy: 63.64%\nEpoch 2/5, Loss: 0.6595460792272789\nEpoch 2/5, Validation Accuracy: 64.06%\nEpoch 3/5, Loss: 0.6540260163459989\nEpoch 3/5, Validation Accuracy: 64.47%\nEpoch 4/5, Loss: 0.645662412129713\nEpoch 4/5, Validation Accuracy: 64.34%\nEpoch 5/5, Loss: 0.6436064426082274\nEpoch 5/5, Validation Accuracy: 65.65%\nFold 3/3\nEpoch 1/5, Loss: 0.634175832758951\nEpoch 1/5, Validation Accuracy: 65.58%\nEpoch 2/5, Loss: 0.6336883276865627\nEpoch 2/5, Validation Accuracy: 67.94%\nEpoch 3/5, Loss: 0.6305777529326592\nEpoch 3/5, Validation Accuracy: 67.87%\nEpoch 4/5, Loss: 0.6281993184959033\nEpoch 4/5, Validation Accuracy: 67.94%\nEpoch 5/5, Loss: 0.6259453556814246\nEpoch 5/5, Validation Accuracy: 67.87%\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-25 22:00:00,367] Trial 2 finished with value: 0.6470452446906739 and parameters: {'lr': 1.9492216631053976e-05}. Best is trial 0 with value: 0.6696675900277009.\n<ipython-input-11-2e749100ab16>:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Fold 1/3\nEpoch 1/5, Loss: 1.0244979789243878\nEpoch 1/5, Validation Accuracy: 60.73%\nEpoch 2/5, Loss: 0.8246411175688327\nEpoch 2/5, Validation Accuracy: 68.98%\nEpoch 3/5, Loss: 1.0155025101498345\nEpoch 3/5, Validation Accuracy: 63.50%\nEpoch 4/5, Loss: 0.9024274360738407\nEpoch 4/5, Validation Accuracy: 65.51%\nEpoch 5/5, Loss: 0.9915180321556428\nEpoch 5/5, Validation Accuracy: 65.51%\nFold 2/3\nEpoch 1/5, Loss: 0.9863850334401947\nEpoch 1/5, Validation Accuracy: 63.99%\nEpoch 2/5, Loss: 1.0436607656887222\nEpoch 2/5, Validation Accuracy: 53.32%\nEpoch 3/5, Loss: 1.0590817129743693\nEpoch 3/5, Validation Accuracy: 65.93%\nEpoch 4/5, Loss: 0.9518634426659642\nEpoch 4/5, Validation Accuracy: 65.79%\nEpoch 5/5, Loss: 1.0224360311558234\nEpoch 5/5, Validation Accuracy: 66.83%\nFold 3/3\nEpoch 1/5, Loss: 1.0510689399683673\nEpoch 1/5, Validation Accuracy: 62.33%\nEpoch 2/5, Loss: 1.0053163170814514\nEpoch 2/5, Validation Accuracy: 63.85%\nEpoch 3/5, Loss: 1.125035048683704\nEpoch 3/5, Validation Accuracy: 69.88%\nEpoch 4/5, Loss: 1.041757961992401\nEpoch 4/5, Validation Accuracy: 55.47%\nEpoch 5/5, Loss: 1.0857245223956873\nEpoch 5/5, Validation Accuracy: 69.46%\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-25 22:24:53,306] Trial 3 finished with value: 0.6708217913204063 and parameters: {'lr': 0.012079301406861595}. Best is trial 3 with value: 0.6708217913204063.\n<ipython-input-11-2e749100ab16>:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Fold 1/3\nEpoch 1/5, Loss: 0.668746625191599\nEpoch 1/5, Validation Accuracy: 64.82%\nEpoch 2/5, Loss: 0.6167525973438558\nEpoch 2/5, Validation Accuracy: 66.14%\nEpoch 3/5, Loss: 0.6007698398927299\nEpoch 3/5, Validation Accuracy: 68.07%\nEpoch 4/5, Loss: 0.605215409842644\nEpoch 4/5, Validation Accuracy: 68.35%\nEpoch 5/5, Loss: 0.5938662056764845\nEpoch 5/5, Validation Accuracy: 69.18%\nFold 2/3\nEpoch 1/5, Loss: 0.5939260708034367\nEpoch 1/5, Validation Accuracy: 70.29%\nEpoch 2/5, Loss: 0.5823503829826966\nEpoch 2/5, Validation Accuracy: 68.84%\nEpoch 3/5, Loss: 0.5856700616976174\nEpoch 3/5, Validation Accuracy: 69.53%\nEpoch 4/5, Loss: 0.5839731368900004\nEpoch 4/5, Validation Accuracy: 70.71%\nEpoch 5/5, Loss: 0.5769858898708174\nEpoch 5/5, Validation Accuracy: 71.47%\nFold 3/3\nEpoch 1/5, Loss: 0.5805896779120956\nEpoch 1/5, Validation Accuracy: 70.36%\nEpoch 2/5, Loss: 0.5773774658777437\nEpoch 2/5, Validation Accuracy: 70.01%\nEpoch 3/5, Loss: 0.5832174372607173\nEpoch 3/5, Validation Accuracy: 70.43%\nEpoch 4/5, Loss: 0.5798956582230099\nEpoch 4/5, Validation Accuracy: 71.33%\nEpoch 5/5, Loss: 0.5687959763885203\nEpoch 5/5, Validation Accuracy: 71.61%\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-25 22:49:44,709] Trial 4 finished with value: 0.6975992613111727 and parameters: {'lr': 0.00029867476169288365}. Best is trial 4 with value: 0.6975992613111727.\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Best Learning Rate for MOBILENET-V2: 0.00029867476169288365\n\nEvaluating on Fold 1/3\nEpoch 1/5, Loss: 0.6639582060318625\nEpoch 1/5, Validation Accuracy: 68.63%\nEpoch 2/5, Loss: 0.6105755337035459\nEpoch 2/5, Validation Accuracy: 69.32%\nEpoch 3/5, Loss: 0.5993891122920737\nEpoch 3/5, Validation Accuracy: 67.59%\nEpoch 4/5, Loss: 0.5986514473488318\nEpoch 4/5, Validation Accuracy: 69.11%\nEpoch 5/5, Loss: 0.5897427514113115\nEpoch 5/5, Validation Accuracy: 68.91%\n\nEvaluating on Fold 2/3\nEpoch 1/5, Loss: 0.595393615039014\nEpoch 1/5, Validation Accuracy: 69.60%\nEpoch 2/5, Loss: 0.6014858747714132\nEpoch 2/5, Validation Accuracy: 71.12%\nEpoch 3/5, Loss: 0.59179246573817\nEpoch 3/5, Validation Accuracy: 69.94%\nEpoch 4/5, Loss: 0.5812959333480392\nEpoch 4/5, Validation Accuracy: 73.27%\nEpoch 5/5, Loss: 0.5862871841501794\nEpoch 5/5, Validation Accuracy: 69.53%\n\nEvaluating on Fold 3/3\nEpoch 1/5, Loss: 0.5821776477342152\nEpoch 1/5, Validation Accuracy: 72.30%\nEpoch 2/5, Loss: 0.5822707329007143\nEpoch 2/5, Validation Accuracy: 70.98%\nEpoch 3/5, Loss: 0.5764864299508089\nEpoch 3/5, Validation Accuracy: 70.57%\nEpoch 4/5, Loss: 0.576374202472729\nEpoch 4/5, Validation Accuracy: 71.40%\nEpoch 5/5, Loss: 0.5761566502942564\nEpoch 5/5, Validation Accuracy: 71.19%\nFold 1 Metrics:\nAccuracy: 0.69, Precision: 0.64, Recall: 0.84, F1-Score: 0.73\nConfusion Matrix:\n[[1002  839]\n [ 279 1489]]\nFold 2 Metrics:\nAccuracy: 0.71, Precision: 0.69, Recall: 0.75, F1-Score: 0.72\nConfusion Matrix:\n[[1211  596]\n [ 456 1346]]\nFold 3 Metrics:\nAccuracy: 0.70, Precision: 0.71, Recall: 0.70, F1-Score: 0.70\nConfusion Matrix:\n[[1228  537]\n [ 557 1286]]\n\nAverage Metrics Across Folds:\nAccuracy: 0.70, Precision: 0.68, Recall: 0.76, F1-Score: 0.72\nConfusion Matrix (sum of all folds):\n[[3441 1972]\n [1292 4121]]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import torch.nn as nn\nimport torchvision.models as models\n\ndef initialize_model(name):\n    if name == \"mobilenet-v2\":\n        model = models.mobilenet_v2(pretrained=True)\n        \n        # Freeze all layers initially\n        for param in model.parameters():\n            param.requires_grad = False\n        \n        # Unfreeze the last convolutional block (features[16]) in MobileNet\n        for param in model.features[16].parameters():\n            param.requires_grad = True\n        \n        # Unfreeze and modify the classifier for binary classification\n        model.classifier[1] = nn.Linear(model.last_channel, 2)\n    \n    else:\n        raise ValueError(\"Model name must be 'mobilenet-v2'\")\n    \n    # Print trainable parameters\n    def count_trainable_params(model):\n        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n    \n    print(f\"Total trainable parameters: {count_trainable_params(model):,}\")\n    \n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T23:15:39.190326Z","iopub.execute_input":"2024-12-25T23:15:39.190584Z","iopub.status.idle":"2024-12-25T23:15:39.195680Z","shell.execute_reply.started":"2024-12-25T23:15:39.190564Z","shell.execute_reply":"2024-12-25T23:15:39.194918Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# for fine tunning\ndef objective(trial, model_name):\n    # Get a suggested learning rate from Optuna\n    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n    \n    # Initialize the model with dropout\n    model = initialize_model(model_name).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n    val_accuracies = []\n    for fold_idx, (train_val_idx, test_idx) in enumerate(kf.split(dataset)):\n        print(f\"Fold {fold_idx + 1}/{num_folds}\")\n        \n        # Create training/validation split\n        train_val_data = Subset(dataset, train_val_idx)\n        test_data = Subset(dataset, test_idx)\n        \n        train_size = int(0.8 * len(train_val_data))\n        val_size = len(train_val_data) - train_size\n        train_data, val_data = torch.utils.data.random_split(\n            train_val_data, [train_size, val_size], generator=torch.Generator().manual_seed(42)\n        )\n        train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n        val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n        \n        # Train the model and get validation accuracy\n        train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs=5)\n        \n        # Evaluate on validation set\n        val_accuracy, _, _, _, _ = calculate_metrics(model, val_loader, device)\n        val_accuracies.append(val_accuracy)\n    \n    # Return the average validation accuracy across all folds as the objective value\n    return np.mean(val_accuracies)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T23:15:39.197031Z","iopub.execute_input":"2024-12-25T23:15:39.197314Z","iopub.status.idle":"2024-12-25T23:15:39.216518Z","shell.execute_reply.started":"2024-12-25T23:15:39.197282Z","shell.execute_reply":"2024-12-25T23:15:39.215735Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def evaluate_test_set(model_name, best_lr):\n    # Initialize model with the best learning rate\n    model = initialize_model(model_name).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=best_lr)\n    criterion = nn.CrossEntropyLoss()\n\n    fold_metrics = []\n    for fold_idx, (train_val_idx, test_idx) in enumerate(kf.split(dataset)):\n        print(f\"\\nEvaluating on Fold {fold_idx + 1}/{num_folds}\")\n        \n        # Create training/validation split\n        train_val_data = Subset(dataset, train_val_idx)\n        test_data = Subset(dataset, test_idx)\n        \n        train_size = int(0.8 * len(train_val_data))\n        val_size = len(train_val_data) - train_size\n        train_data, val_data = torch.utils.data.random_split(\n            train_val_data, [train_size, val_size], generator=torch.Generator().manual_seed(42)\n        )\n        train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n        val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n        \n        # Train the model\n        train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs=5)\n        \n        # Evaluate on the test set\n        test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n        fold_metrics.append(calculate_metrics(model, test_loader, device))\n    \n    # Print metrics for each fold\n    for fold_idx, metrics in enumerate(fold_metrics):\n        accuracy, precision, recall, f1, conf_matrix = metrics\n        print(f\"Fold {fold_idx + 1} Metrics:\")\n        print(f\"Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}\")\n        print(f\"Confusion Matrix:\\n{conf_matrix}\")\n\n# Calculate average metrics across folds\n    avg_accuracy = np.mean([metrics[0] for metrics in fold_metrics])\n    avg_precision = np.mean([metrics[1] for metrics in fold_metrics])\n    avg_recall = np.mean([metrics[2] for metrics in fold_metrics])\n    avg_f1 = np.mean([metrics[3] for metrics in fold_metrics])\n    total_conf_matrix = np.sum([metrics[4] for metrics in fold_metrics], axis=0)\n\n    print(\"\\nAverage Metrics Across Folds:\")\n    print(f\"Accuracy: {avg_accuracy:.2f}, Precision: {avg_precision:.2f}, Recall: {avg_recall:.2f}, F1-Score: {avg_f1:.2f}\")\n    print(f\"Confusion Matrix (sum of all folds):\\n{total_conf_matrix}\")\n\n\n# Optuna Optimization and Final Testing\nfor model_name in [\"mobilenet-v2\"]:\n    print(f\"\\nOptimizing for {model_name.upper()}...\")\n    study = optuna.create_study(direction='maximize')\n    study.optimize(lambda trial: objective(trial, model_name), n_trials=5)  # You can increase the number of trials if needed\n\n    # Best learning rate found for the model\n    best_lr = study.best_params['lr']\n    print(f\"Best Learning Rate for {model_name.upper()}: {best_lr}\")\n\n    # Evaluate on test sets for each fold\n    evaluate_test_set(model_name, best_lr)\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T23:24:30.800196Z","iopub.execute_input":"2024-12-25T23:24:30.800552Z","iopub.status.idle":"2024-12-26T01:56:50.691118Z","shell.execute_reply.started":"2024-12-25T23:24:30.800525Z","shell.execute_reply":"2024-12-26T01:56:50.690276Z"}},"outputs":[{"name":"stderr","text":"[I 2024-12-25 23:24:30,806] A new study created in memory with name: no-name-76ae84c6-f21c-4d78-b20d-b966d49dc86b\n","output_type":"stream"},{"name":"stdout","text":"\nOptimizing for MOBILENET-V2...\nTotal trainable parameters: 322,562\nFold 1/3\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-14-1c309b109a5c>:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Loss: 0.6995144532530347\nEpoch 1/5, Validation Accuracy: 59.83%\nEpoch 2/5, Loss: 0.6582492543847521\nEpoch 2/5, Validation Accuracy: 63.92%\nEpoch 3/5, Loss: 0.6402103229122267\nEpoch 3/5, Validation Accuracy: 67.66%\nEpoch 4/5, Loss: 0.6177715537297791\nEpoch 4/5, Validation Accuracy: 68.21%\nEpoch 5/5, Loss: 0.5995829452796536\nEpoch 5/5, Validation Accuracy: 69.88%\nFold 2/3\nEpoch 1/5, Loss: 0.5932482262013367\nEpoch 1/5, Validation Accuracy: 70.57%\nEpoch 2/5, Loss: 0.5760339319047348\nEpoch 2/5, Validation Accuracy: 71.33%\nEpoch 3/5, Loss: 0.5687125142437318\nEpoch 3/5, Validation Accuracy: 73.13%\nEpoch 4/5, Loss: 0.5528306162489053\nEpoch 4/5, Validation Accuracy: 71.95%\nEpoch 5/5, Loss: 0.5497544327164223\nEpoch 5/5, Validation Accuracy: 72.37%\nFold 3/3\nEpoch 1/5, Loss: 0.5400752843414223\nEpoch 1/5, Validation Accuracy: 73.06%\nEpoch 2/5, Loss: 0.5350997797033404\nEpoch 2/5, Validation Accuracy: 73.20%\nEpoch 3/5, Loss: 0.5283351934087869\nEpoch 3/5, Validation Accuracy: 74.10%\nEpoch 4/5, Loss: 0.5204209716609829\nEpoch 4/5, Validation Accuracy: 74.38%\nEpoch 5/5, Loss: 0.5217725934244651\nEpoch 5/5, Validation Accuracy: 73.27%\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-25 23:49:49,493] Trial 0 finished with value: 0.7301477377654663 and parameters: {'lr': 1.3036571335702671e-05}. Best is trial 0 with value: 0.7301477377654663.\n<ipython-input-14-1c309b109a5c>:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Total trainable parameters: 322,562\nFold 1/3\nEpoch 1/5, Loss: 0.5688418500660533\nEpoch 1/5, Validation Accuracy: 73.13%\nEpoch 2/5, Loss: 0.5004152785677937\nEpoch 2/5, Validation Accuracy: 74.31%\nEpoch 3/5, Loss: 0.4640498283159667\nEpoch 3/5, Validation Accuracy: 78.74%\nEpoch 4/5, Loss: 0.4516609766371342\nEpoch 4/5, Validation Accuracy: 77.42%\nEpoch 5/5, Loss: 0.43846709943937334\nEpoch 5/5, Validation Accuracy: 78.05%\nFold 2/3\nEpoch 1/5, Loss: 0.4579309697638559\nEpoch 1/5, Validation Accuracy: 77.56%\nEpoch 2/5, Loss: 0.4321066669666965\nEpoch 2/5, Validation Accuracy: 80.61%\nEpoch 3/5, Loss: 0.4139901256363695\nEpoch 3/5, Validation Accuracy: 81.79%\nEpoch 4/5, Loss: 0.4119853753575963\nEpoch 4/5, Validation Accuracy: 80.47%\nEpoch 5/5, Loss: 0.3951178384913924\nEpoch 5/5, Validation Accuracy: 80.40%\nFold 3/3\nEpoch 1/5, Loss: 0.40735956063257395\nEpoch 1/5, Validation Accuracy: 82.69%\nEpoch 2/5, Loss: 0.39802654071078114\nEpoch 2/5, Validation Accuracy: 82.76%\nEpoch 3/5, Loss: 0.393764010450458\nEpoch 3/5, Validation Accuracy: 82.34%\nEpoch 4/5, Loss: 0.37226241563565166\nEpoch 4/5, Validation Accuracy: 82.13%\nEpoch 5/5, Loss: 0.36826269270965406\nEpoch 5/5, Validation Accuracy: 82.13%\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-26 00:14:49,996] Trial 1 finished with value: 0.8000923361034165 and parameters: {'lr': 0.0005358852134223212}. Best is trial 1 with value: 0.8000923361034165.\n<ipython-input-14-1c309b109a5c>:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Total trainable parameters: 322,562\nFold 1/3\nEpoch 1/5, Loss: 1.295462268986096\nEpoch 1/5, Validation Accuracy: 70.29%\nEpoch 2/5, Loss: 0.8565546547510348\nEpoch 2/5, Validation Accuracy: 70.78%\nEpoch 3/5, Loss: 0.7718939978773423\nEpoch 3/5, Validation Accuracy: 69.53%\nEpoch 4/5, Loss: 0.7612238066960435\nEpoch 4/5, Validation Accuracy: 64.20%\nEpoch 5/5, Loss: 0.7500767628790924\nEpoch 5/5, Validation Accuracy: 67.66%\nFold 2/3\nEpoch 1/5, Loss: 0.7680169090381643\nEpoch 1/5, Validation Accuracy: 54.16%\nEpoch 2/5, Loss: 0.8457812943511246\nEpoch 2/5, Validation Accuracy: 73.06%\nEpoch 3/5, Loss: 0.8119724602824416\nEpoch 3/5, Validation Accuracy: 65.24%\nEpoch 4/5, Loss: 0.8513665247029363\nEpoch 4/5, Validation Accuracy: 69.88%\nEpoch 5/5, Loss: 0.8791207521330586\nEpoch 5/5, Validation Accuracy: 69.04%\nFold 3/3\nEpoch 1/5, Loss: 0.939784914419796\nEpoch 1/5, Validation Accuracy: 65.44%\nEpoch 2/5, Loss: 1.0011298228363965\nEpoch 2/5, Validation Accuracy: 69.81%\nEpoch 3/5, Loss: 0.9520354975652958\nEpoch 3/5, Validation Accuracy: 70.50%\nEpoch 4/5, Loss: 0.8734488793499562\nEpoch 4/5, Validation Accuracy: 61.70%\nEpoch 5/5, Loss: 0.95077241529088\nEpoch 5/5, Validation Accuracy: 65.37%\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-26 00:40:13,980] Trial 2 finished with value: 0.6710526315789475 and parameters: {'lr': 0.03533549148188352}. Best is trial 1 with value: 0.8000923361034165.\n<ipython-input-14-1c309b109a5c>:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Total trainable parameters: 322,562\nFold 1/3\nEpoch 1/5, Loss: 1.4852922669925743\nEpoch 1/5, Validation Accuracy: 68.21%\nEpoch 2/5, Loss: 1.1162220396060312\nEpoch 2/5, Validation Accuracy: 72.58%\nEpoch 3/5, Loss: 0.765747710650797\nEpoch 3/5, Validation Accuracy: 65.65%\nEpoch 4/5, Loss: 0.7320874786179369\nEpoch 4/5, Validation Accuracy: 72.09%\nEpoch 5/5, Loss: 0.769423152860357\nEpoch 5/5, Validation Accuracy: 53.81%\nFold 2/3\nEpoch 1/5, Loss: 0.8847812250338866\nEpoch 1/5, Validation Accuracy: 66.97%\nEpoch 2/5, Loss: 0.9314551176451846\nEpoch 2/5, Validation Accuracy: 72.37%\nEpoch 3/5, Loss: 0.9598060039525532\nEpoch 3/5, Validation Accuracy: 51.25%\nEpoch 4/5, Loss: 0.9930416554706532\nEpoch 4/5, Validation Accuracy: 55.12%\nEpoch 5/5, Loss: 0.9983612378822506\nEpoch 5/5, Validation Accuracy: 59.28%\nFold 3/3\nEpoch 1/5, Loss: 1.149370577042274\nEpoch 1/5, Validation Accuracy: 54.43%\nEpoch 2/5, Loss: 1.0799398407422376\nEpoch 2/5, Validation Accuracy: 48.55%\nEpoch 3/5, Loss: 1.0893226037038624\nEpoch 3/5, Validation Accuracy: 49.03%\nEpoch 4/5, Loss: 1.0394062245089706\nEpoch 4/5, Validation Accuracy: 55.75%\nEpoch 5/5, Loss: 0.8510047368581782\nEpoch 5/5, Validation Accuracy: 70.01%\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-26 01:05:03,020] Trial 3 finished with value: 0.5967220683287165 and parameters: {'lr': 0.03804465934988528}. Best is trial 1 with value: 0.8000923361034165.\n<ipython-input-14-1c309b109a5c>:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Total trainable parameters: 322,562\nFold 1/3\nEpoch 1/5, Loss: 1.8221591552976746\nEpoch 1/5, Validation Accuracy: 70.91%\nEpoch 2/5, Loss: 1.057603949341326\nEpoch 2/5, Validation Accuracy: 61.08%\nEpoch 3/5, Loss: 0.9654474584437207\nEpoch 3/5, Validation Accuracy: 67.87%\nEpoch 4/5, Loss: 0.8432630025550146\nEpoch 4/5, Validation Accuracy: 71.61%\nEpoch 5/5, Loss: 0.8959910195341426\nEpoch 5/5, Validation Accuracy: 64.89%\nFold 2/3\nEpoch 1/5, Loss: 0.9700871434330282\nEpoch 1/5, Validation Accuracy: 53.88%\nEpoch 2/5, Loss: 0.9702557213398633\nEpoch 2/5, Validation Accuracy: 70.08%\nEpoch 3/5, Loss: 0.8536625838740755\nEpoch 3/5, Validation Accuracy: 68.49%\nEpoch 4/5, Loss: 0.9688798864243439\nEpoch 4/5, Validation Accuracy: 68.28%\nEpoch 5/5, Loss: 1.0540350229371318\nEpoch 5/5, Validation Accuracy: 50.62%\nFold 3/3\nEpoch 1/5, Loss: 0.9882221877245613\nEpoch 1/5, Validation Accuracy: 55.12%\nEpoch 2/5, Loss: 0.988232741533722\nEpoch 2/5, Validation Accuracy: 66.83%\nEpoch 3/5, Loss: 0.9855969133298041\nEpoch 3/5, Validation Accuracy: 52.77%\nEpoch 4/5, Loss: 1.256925272842797\nEpoch 4/5, Validation Accuracy: 68.91%\nEpoch 5/5, Loss: 1.191562942707736\nEpoch 5/5, Validation Accuracy: 68.98%\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-26 01:30:19,279] Trial 4 finished with value: 0.6048014773776546 and parameters: {'lr': 0.04536774544628303}. Best is trial 1 with value: 0.8000923361034165.\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Best Learning Rate for MOBILENET-V2: 0.0005358852134223212\nTotal trainable parameters: 322,562\n\nEvaluating on Fold 1/3\nEpoch 1/5, Loss: 0.5749680057414988\nEpoch 1/5, Validation Accuracy: 74.65%\nEpoch 2/5, Loss: 0.5109964225502962\nEpoch 2/5, Validation Accuracy: 74.03%\nEpoch 3/5, Loss: 0.4681811415852763\nEpoch 3/5, Validation Accuracy: 77.15%\nEpoch 4/5, Loss: 0.44557433423087084\nEpoch 4/5, Validation Accuracy: 77.08%\nEpoch 5/5, Loss: 0.4444709707524895\nEpoch 5/5, Validation Accuracy: 78.39%\n\nEvaluating on Fold 2/3\nEpoch 1/5, Loss: 0.4317691458687598\nEpoch 1/5, Validation Accuracy: 78.32%\nEpoch 2/5, Loss: 0.42117531539985487\nEpoch 2/5, Validation Accuracy: 80.06%\nEpoch 3/5, Loss: 0.40551101947357643\nEpoch 3/5, Validation Accuracy: 79.09%\nEpoch 4/5, Loss: 0.4172816409919802\nEpoch 4/5, Validation Accuracy: 76.18%\nEpoch 5/5, Loss: 0.39732762660769466\nEpoch 5/5, Validation Accuracy: 82.41%\n\nEvaluating on Fold 3/3\nEpoch 1/5, Loss: 0.4047333216963552\nEpoch 1/5, Validation Accuracy: 81.65%\nEpoch 2/5, Loss: 0.39501723004968126\nEpoch 2/5, Validation Accuracy: 82.06%\nEpoch 3/5, Loss: 0.3881127287669735\nEpoch 3/5, Validation Accuracy: 82.96%\nEpoch 4/5, Loss: 0.3710442018278396\nEpoch 4/5, Validation Accuracy: 81.79%\nEpoch 5/5, Loss: 0.38542182035538375\nEpoch 5/5, Validation Accuracy: 84.90%\nFold 1 Metrics:\nAccuracy: 0.80, Precision: 0.80, Recall: 0.79, F1-Score: 0.79\nConfusion Matrix:\n[[1485  356]\n [ 380 1388]]\nFold 2 Metrics:\nAccuracy: 0.82, Precision: 0.81, Recall: 0.83, F1-Score: 0.82\nConfusion Matrix:\n[[1461  346]\n [ 302 1500]]\nFold 3 Metrics:\nAccuracy: 0.84, Precision: 0.84, Recall: 0.85, F1-Score: 0.84\nConfusion Matrix:\n[[1457  308]\n [ 277 1566]]\n\nAverage Metrics Across Folds:\nAccuracy: 0.82, Precision: 0.81, Recall: 0.82, F1-Score: 0.82\nConfusion Matrix (sum of all folds):\n[[4403 1010]\n [ 959 4454]]\n","output_type":"stream"}],"execution_count":16}]}