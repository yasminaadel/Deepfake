{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10297968,"sourceType":"datasetVersion","datasetId":6373963}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torchvision import transforms, models, datasets\nimport torch.optim as optim\nimport torch.nn as nn\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, precision_recall_fscore_support)\nfrom transformers import ViTForImageClassification, ViTFeatureExtractor\nimport optuna\nfrom torchvision.datasets import ImageFolder","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-26T07:25:29.511016Z","iopub.execute_input":"2024-12-26T07:25:29.511289Z","iopub.status.idle":"2024-12-26T07:25:44.591949Z","shell.execute_reply.started":"2024-12-26T07:25:29.511269Z","shell.execute_reply":"2024-12-26T07:25:44.591250Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"dataset_dir ='/kaggle/input/deepfake/DeepFake'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T07:25:46.816343Z","iopub.execute_input":"2024-12-26T07:25:46.816935Z","iopub.status.idle":"2024-12-26T07:25:46.820382Z","shell.execute_reply.started":"2024-12-26T07:25:46.816906Z","shell.execute_reply":"2024-12-26T07:25:46.819545Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Transformations for training, validation, and testing\ntransform_train = transforms.Compose([\n    transforms.Resize((299, 299)),  \n    transforms.ToTensor(),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(p=0.2),\n    transforms.RandomRotation(15),\n    transforms.RandomCrop(299, padding=10),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.RandomAffine(degrees=20, scale=(0.8, 1.2), shear=10),\n    transforms.RandomErasing(p=0.3),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\ntransform_val_test = transforms.Compose([\n    transforms.Resize((299, 299)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T07:25:52.062110Z","iopub.execute_input":"2024-12-26T07:25:52.062412Z","iopub.status.idle":"2024-12-26T07:25:52.068627Z","shell.execute_reply.started":"2024-12-26T07:25:52.062385Z","shell.execute_reply":"2024-12-26T07:25:52.067676Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Load the dataset\ndataset = ImageFolder(root=dataset_dir, transform=transform_train)\nprint(\"Classes:\", dataset.classes)\nprint(\"Class-to-Index Mapping:\", dataset.class_to_idx)\nprint(\"Number of Samples:\", len(dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T07:25:55.812311Z","iopub.execute_input":"2024-12-26T07:25:55.812579Z","iopub.status.idle":"2024-12-26T07:26:02.747388Z","shell.execute_reply.started":"2024-12-26T07:25:55.812559Z","shell.execute_reply":"2024-12-26T07:26:02.746545Z"}},"outputs":[{"name":"stdout","text":"Classes: ['Fake', 'Real']\nClass-to-Index Mapping: {'Fake': 0, 'Real': 1}\nNumber of Samples: 10826\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Function to get the Inception model (without fine-tuning)\ndef get_model(model_name):\n    if model_name == \"inception-v3\":\n        model = models.inception_v3(pretrained=True, aux_logits=True)\n\n        # Freeze all layers\n        for param in model.parameters():\n            param.requires_grad = False\n\n        # Modify the final fully connected layer for binary classification\n        model.fc = nn.Linear(model.fc.in_features, 2) \n\n        return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T19:15:36.381445Z","iopub.execute_input":"2024-12-25T19:15:36.381783Z","iopub.status.idle":"2024-12-25T19:15:36.386308Z","shell.execute_reply.started":"2024-12-25T19:15:36.381758Z","shell.execute_reply":"2024-12-25T19:15:36.385571Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Calculate metrics function\ndef calculate_metrics(model, loader, device):\n    \n    # Set the model to evaluation mode (disables dropout)\n    model.eval()\n\n    # Lists to store true labels and predicted labels\n    all_labels = []\n    all_predictions = []\n\n    # Disabling gradient computation\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n             # Get predicted labels by taking the argmax (most likely class)\n            _, predicted = torch.max(outputs, 1)\n            all_labels.extend(labels.cpu().numpy())\n            all_predictions.extend(predicted.cpu().numpy())\n\n     # Calculate the confusion matrix,which give TN, FP, FN, and TP\n    conf_matrix = confusion_matrix(all_labels, all_predictions)\n    # Unpack the confusion matrix into four components: TN, FP, FN, TP\n    TN, FP, FN, TP = conf_matrix.ravel() \n\n    total = conf_matrix.sum()\n    accuracy = (TP + TN) / total if total > 0 else 0.0\n    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n    recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n    \n    return accuracy, precision, recall, f1, conf_matrix\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T07:26:09.756931Z","iopub.execute_input":"2024-12-26T07:26:09.757332Z","iopub.status.idle":"2024-12-26T07:26:09.766492Z","shell.execute_reply.started":"2024-12-26T07:26:09.757299Z","shell.execute_reply":"2024-12-26T07:26:09.764886Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Train the model function with validation accuracy printed after each epoch\ndef train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs=5):\n    # Variable to track the best validation accuracy\n    best_val_accuracy = 0\n    \n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n         # Iterate over batches in the training data\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(images)\n\n            # If the output is a tuple (e.g., from Inception model), get the logits\n            if isinstance(outputs, tuple):\n                outputs = outputs[0]\n                \n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n        \n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n        # Validation phase\n        model.eval()\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n\n                 # If the output is a tuple (e.g., from Inception model), get the logits\n                if isinstance(outputs, tuple):\n                    outputs = outputs[0]\n                    \n                _, predicted = torch.max(outputs, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n        \n        val_accuracy = 100 * correct / total\n        print(f\"Epoch {epoch+1}/{epochs}, Validation Accuracy: {val_accuracy:.2f}%\")\n        \n        if val_accuracy > best_val_accuracy:\n            best_val_accuracy = val_accuracy\n    \n    return best_val_accuracy\n\n# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Cross-validation setup\nnum_folds = 3\nkf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T07:26:14.566315Z","iopub.execute_input":"2024-12-26T07:26:14.566583Z","iopub.status.idle":"2024-12-26T07:26:14.649250Z","shell.execute_reply.started":"2024-12-26T07:26:14.566562Z","shell.execute_reply":"2024-12-26T07:26:14.648251Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def objective(trial, model_name):\n    # Get a suggested learning rate from Optuna\n    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n    \n    # Initialize the model with dropout\n    model = get_model(model_name).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n    val_accuracies = []\n    for fold_idx, (train_val_idx, test_idx) in enumerate(kf.split(dataset)):\n        print(f\"Fold {fold_idx + 1}/{num_folds}\")\n        \n        # Create training/validation split\n        train_val_data = Subset(dataset, train_val_idx)\n        test_data = Subset(dataset, test_idx)\n        \n        train_size = int(0.8 * len(train_val_data))\n        val_size = len(train_val_data) - train_size\n        train_data, val_data = torch.utils.data.random_split(\n            train_val_data, [train_size, val_size], generator=torch.Generator().manual_seed(42)\n        )\n        train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n        val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n        \n        # Train the model and get validation accuracy\n        train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs=5)\n        \n        # Evaluate on validation set\n        val_accuracy, _, _, _, _ = calculate_metrics(model, val_loader, device)\n        val_accuracies.append(val_accuracy)\n    \n    # Return the average validation accuracy across all folds as the objective value\n    return np.mean(val_accuracies)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T19:20:09.470114Z","iopub.execute_input":"2024-12-25T19:20:09.470411Z","iopub.status.idle":"2024-12-25T19:20:09.476371Z","shell.execute_reply.started":"2024-12-25T19:20:09.470389Z","shell.execute_reply":"2024-12-25T19:20:09.475480Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def evaluate_test_set(model_name, best_lr):\n    # Initialize model with the best learning rate\n    model = get_model(model_name).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=best_lr)\n    criterion = nn.CrossEntropyLoss()\n\n    fold_metrics = []\n    for fold_idx, (train_val_idx, test_idx) in enumerate(kf.split(dataset)):\n        print(f\"\\nEvaluating on Fold {fold_idx + 1}/{num_folds}\")\n        \n        # Create training/validation split\n        train_val_data = Subset(dataset, train_val_idx)\n        test_data = Subset(dataset, test_idx)\n        \n        train_size = int(0.8 * len(train_val_data))\n        val_size = len(train_val_data) - train_size\n        train_data, val_data = torch.utils.data.random_split(\n            train_val_data, [train_size, val_size], generator=torch.Generator().manual_seed(42)\n        )\n        train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n        val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n        \n        # Train the model\n        train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs=5)\n        \n        # Evaluate on the test set\n        test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n        fold_metrics.append(calculate_metrics(model, test_loader, device))\n    \n    # Print metrics for each fold\n    for fold_idx, metrics in enumerate(fold_metrics):\n        accuracy, precision, recall, f1, conf_matrix = metrics\n        print(f\"Fold {fold_idx + 1} Metrics:\")\n        print(f\"Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}\")\n        print(f\"Confusion Matrix:\\n{conf_matrix}\")\n\n# Calculate average metrics across folds\n    avg_accuracy = np.mean([metrics[0] for metrics in fold_metrics])\n    avg_precision = np.mean([metrics[1] for metrics in fold_metrics])\n    avg_recall = np.mean([metrics[2] for metrics in fold_metrics])\n    avg_f1 = np.mean([metrics[3] for metrics in fold_metrics])\n    total_conf_matrix = np.sum([metrics[4] for metrics in fold_metrics], axis=0)\n\n    print(\"\\nAverage Metrics Across Folds:\")\n    print(f\"Accuracy: {avg_accuracy:.2f}, Precision: {avg_precision:.2f}, Recall: {avg_recall:.2f}, F1-Score: {avg_f1:.2f}\")\n    print(f\"Confusion Matrix (sum of all folds):\\n{total_conf_matrix}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T19:20:11.767183Z","iopub.execute_input":"2024-12-25T19:20:11.767502Z","iopub.status.idle":"2024-12-25T19:20:11.775336Z","shell.execute_reply.started":"2024-12-25T19:20:11.767473Z","shell.execute_reply":"2024-12-25T19:20:11.774420Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"for model_name in [\"inception-v3\"]:\n    print(f\"\\nOptimizing for {model_name.upper()}...\")\n    study = optuna.create_study(direction='maximize')\n    study.optimize(lambda trial: objective(trial, model_name), n_trials=5) \n\n    # Best learning rate found for the model\n    best_lr = study.best_params['lr']\n    print(f\"Best Learning Rate for {model_name.upper()}: {best_lr}\")\n\n    # Evaluate on test sets for each fold\n    evaluate_test_set(model_name, best_lr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T19:20:14.214264Z","iopub.execute_input":"2024-12-25T19:20:14.214580Z","iopub.status.idle":"2024-12-25T23:35:46.984486Z","shell.execute_reply.started":"2024-12-25T19:20:14.214531Z","shell.execute_reply":"2024-12-25T23:35:46.983597Z"}},"outputs":[{"name":"stderr","text":"[I 2024-12-25 19:20:14,216] A new study created in memory with name: no-name-c0f869cb-61cf-4757-861b-c389146b4253\n","output_type":"stream"},{"name":"stdout","text":"\nOptimizing for INCEPTION-V3...\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-19-2e749100ab16>:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n","output_type":"stream"},{"name":"stdout","text":"Fold 1/3\nEpoch 1/5, Loss: 2.795867316630664\nEpoch 1/5, Validation Accuracy: 66.69%\nEpoch 2/5, Loss: 3.2300137880757367\nEpoch 2/5, Validation Accuracy: 56.99%\nEpoch 3/5, Loss: 3.430133197189036\nEpoch 3/5, Validation Accuracy: 66.48%\nEpoch 4/5, Loss: 4.059932265492434\nEpoch 4/5, Validation Accuracy: 54.85%\nEpoch 5/5, Loss: 3.7087764647784156\nEpoch 5/5, Validation Accuracy: 60.80%\nFold 2/3\nEpoch 1/5, Loss: 3.5473217621692634\nEpoch 1/5, Validation Accuracy: 65.79%\nEpoch 2/5, Loss: 3.5817354818734017\nEpoch 2/5, Validation Accuracy: 63.37%\nEpoch 3/5, Loss: 4.040954607626351\nEpoch 3/5, Validation Accuracy: 66.97%\nEpoch 4/5, Loss: 3.677936596106429\nEpoch 4/5, Validation Accuracy: 67.11%\nEpoch 5/5, Loss: 3.6948164279948283\nEpoch 5/5, Validation Accuracy: 55.96%\nFold 3/3\nEpoch 1/5, Loss: 4.191949609234847\nEpoch 1/5, Validation Accuracy: 67.66%\nEpoch 2/5, Loss: 4.482568564994559\nEpoch 2/5, Validation Accuracy: 64.47%\nEpoch 3/5, Loss: 3.631193316443849\nEpoch 3/5, Validation Accuracy: 66.90%\nEpoch 4/5, Loss: 3.589639568855749\nEpoch 4/5, Validation Accuracy: 63.99%\nEpoch 5/5, Loss: 4.340351416261156\nEpoch 5/5, Validation Accuracy: 68.49%\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-25 20:03:12,090] Trial 0 finished with value: 0.605724838411819 and parameters: {'lr': 0.04249100649730569}. Best is trial 0 with value: 0.605724838411819.\n<ipython-input-19-2e749100ab16>:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Fold 1/3\nEpoch 1/5, Loss: 3.0075070021560837\nEpoch 1/5, Validation Accuracy: 61.36%\nEpoch 2/5, Loss: 2.804308584052555\nEpoch 2/5, Validation Accuracy: 54.64%\nEpoch 3/5, Loss: 3.42648641048874\nEpoch 3/5, Validation Accuracy: 56.44%\nEpoch 4/5, Loss: 3.8972610561228587\nEpoch 4/5, Validation Accuracy: 67.73%\nEpoch 5/5, Loss: 3.6159748425141225\nEpoch 5/5, Validation Accuracy: 63.99%\nFold 2/3\nEpoch 1/5, Loss: 3.7845103431143157\nEpoch 1/5, Validation Accuracy: 65.72%\nEpoch 2/5, Loss: 3.426114917789375\nEpoch 2/5, Validation Accuracy: 62.81%\nEpoch 3/5, Loss: 4.173008325350219\nEpoch 3/5, Validation Accuracy: 63.23%\nEpoch 4/5, Loss: 3.5900464802157153\nEpoch 4/5, Validation Accuracy: 63.30%\nEpoch 5/5, Loss: 4.176364876288735\nEpoch 5/5, Validation Accuracy: 67.31%\nFold 3/3\nEpoch 1/5, Loss: 3.48214399649952\nEpoch 1/5, Validation Accuracy: 68.14%\nEpoch 2/5, Loss: 3.622087696968521\nEpoch 2/5, Validation Accuracy: 67.59%\nEpoch 3/5, Loss: 3.6166993534367387\nEpoch 3/5, Validation Accuracy: 57.89%\nEpoch 4/5, Loss: 3.683364896155194\nEpoch 4/5, Validation Accuracy: 67.52%\nEpoch 5/5, Loss: 3.5198840798594015\nEpoch 5/5, Validation Accuracy: 58.80%\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-25 20:46:35,635] Trial 1 finished with value: 0.6193444136657433 and parameters: {'lr': 0.04239362000105665}. Best is trial 1 with value: 0.6193444136657433.\n<ipython-input-19-2e749100ab16>:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Fold 1/3\nEpoch 1/5, Loss: 0.6892281994635229\nEpoch 1/5, Validation Accuracy: 59.70%\nEpoch 2/5, Loss: 0.664614517056481\nEpoch 2/5, Validation Accuracy: 64.06%\nEpoch 3/5, Loss: 0.6448811974314695\nEpoch 3/5, Validation Accuracy: 64.89%\nEpoch 4/5, Loss: 0.6342980321599634\nEpoch 4/5, Validation Accuracy: 65.58%\nEpoch 5/5, Loss: 0.6261299234398162\nEpoch 5/5, Validation Accuracy: 66.97%\nFold 2/3\nEpoch 1/5, Loss: 0.6220047898714055\nEpoch 1/5, Validation Accuracy: 69.74%\nEpoch 2/5, Loss: 0.620251307006699\nEpoch 2/5, Validation Accuracy: 68.07%\nEpoch 3/5, Loss: 0.616147080508385\nEpoch 3/5, Validation Accuracy: 66.69%\nEpoch 4/5, Loss: 0.6202073719620046\nEpoch 4/5, Validation Accuracy: 70.22%\nEpoch 5/5, Loss: 0.6108420128980394\nEpoch 5/5, Validation Accuracy: 69.81%\nFold 3/3\nEpoch 1/5, Loss: 0.6048471870014022\nEpoch 1/5, Validation Accuracy: 69.74%\nEpoch 2/5, Loss: 0.6021964826636551\nEpoch 2/5, Validation Accuracy: 70.36%\nEpoch 3/5, Loss: 0.6051977242224783\nEpoch 3/5, Validation Accuracy: 71.19%\nEpoch 4/5, Loss: 0.6086463220211682\nEpoch 4/5, Validation Accuracy: 70.78%\nEpoch 5/5, Loss: 0.6029919542331064\nEpoch 5/5, Validation Accuracy: 70.57%\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-25 21:28:05,352] Trial 2 finished with value: 0.6793628808864266 and parameters: {'lr': 0.0001153980122372495}. Best is trial 2 with value: 0.6793628808864266.\n<ipython-input-19-2e749100ab16>:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Fold 1/3\nEpoch 1/5, Loss: 1.5928743642996688\nEpoch 1/5, Validation Accuracy: 59.90%\nEpoch 2/5, Loss: 1.4479668423615766\nEpoch 2/5, Validation Accuracy: 62.81%\nEpoch 3/5, Loss: 1.904955631132284\nEpoch 3/5, Validation Accuracy: 56.99%\nEpoch 4/5, Loss: 1.7519627690973862\nEpoch 4/5, Validation Accuracy: 54.22%\nEpoch 5/5, Loss: 1.5898829286928335\nEpoch 5/5, Validation Accuracy: 65.93%\nFold 2/3\nEpoch 1/5, Loss: 1.5696268252904904\nEpoch 1/5, Validation Accuracy: 50.14%\nEpoch 2/5, Loss: 1.8187278851619741\nEpoch 2/5, Validation Accuracy: 63.43%\nEpoch 3/5, Loss: 1.6177349829871353\nEpoch 3/5, Validation Accuracy: 67.73%\nEpoch 4/5, Loss: 1.9247395043873656\nEpoch 4/5, Validation Accuracy: 52.29%\nEpoch 5/5, Loss: 1.6977640375577283\nEpoch 5/5, Validation Accuracy: 59.76%\nFold 3/3\nEpoch 1/5, Loss: 1.6965398628738044\nEpoch 1/5, Validation Accuracy: 53.74%\nEpoch 2/5, Loss: 2.3102478121525674\nEpoch 2/5, Validation Accuracy: 67.31%\nEpoch 3/5, Loss: 1.7698275776199215\nEpoch 3/5, Validation Accuracy: 63.85%\nEpoch 4/5, Loss: 1.7271507728165685\nEpoch 4/5, Validation Accuracy: 61.77%\nEpoch 5/5, Loss: 1.7657877802848816\nEpoch 5/5, Validation Accuracy: 65.79%\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-25 22:09:37,990] Trial 3 finished with value: 0.6435826408125577 and parameters: {'lr': 0.018597218691203208}. Best is trial 2 with value: 0.6793628808864266.\n<ipython-input-19-2e749100ab16>:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Fold 1/3\nEpoch 1/5, Loss: 0.7136827671066832\nEpoch 1/5, Validation Accuracy: 49.03%\nEpoch 2/5, Loss: 0.7057722158194906\nEpoch 2/5, Validation Accuracy: 50.76%\nEpoch 3/5, Loss: 0.7061050623161358\nEpoch 3/5, Validation Accuracy: 54.29%\nEpoch 4/5, Loss: 0.6980732695832437\nEpoch 4/5, Validation Accuracy: 52.84%\nEpoch 5/5, Loss: 0.6953908346634543\nEpoch 5/5, Validation Accuracy: 55.26%\nFold 2/3\nEpoch 1/5, Loss: 0.6909991947326871\nEpoch 1/5, Validation Accuracy: 59.07%\nEpoch 2/5, Loss: 0.6908998690257415\nEpoch 2/5, Validation Accuracy: 61.50%\nEpoch 3/5, Loss: 0.6844472983924065\nEpoch 3/5, Validation Accuracy: 58.10%\nEpoch 4/5, Loss: 0.6806260870965146\nEpoch 4/5, Validation Accuracy: 61.15%\nEpoch 5/5, Loss: 0.6813216667148948\nEpoch 5/5, Validation Accuracy: 61.98%\nFold 3/3\nEpoch 1/5, Loss: 0.6774347697832308\nEpoch 1/5, Validation Accuracy: 63.57%\nEpoch 2/5, Loss: 0.6723154977540284\nEpoch 2/5, Validation Accuracy: 60.66%\nEpoch 3/5, Loss: 0.6724251818261753\nEpoch 3/5, Validation Accuracy: 63.78%\nEpoch 4/5, Loss: 0.671047273261771\nEpoch 4/5, Validation Accuracy: 61.50%\nEpoch 5/5, Loss: 0.6609282068784724\nEpoch 5/5, Validation Accuracy: 62.53%\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-25 22:51:14,567] Trial 4 finished with value: 0.6126500461680517 and parameters: {'lr': 1.1463590907212715e-05}. Best is trial 2 with value: 0.6793628808864266.\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Best Learning Rate for INCEPTION-V3: 0.0001153980122372495\n\nEvaluating on Fold 1/3\nEpoch 1/5, Loss: 0.6924630357415636\nEpoch 1/5, Validation Accuracy: 62.33%\nEpoch 2/5, Loss: 0.6629610618175064\nEpoch 2/5, Validation Accuracy: 65.44%\nEpoch 3/5, Loss: 0.6446246784694946\nEpoch 3/5, Validation Accuracy: 65.37%\nEpoch 4/5, Loss: 0.6363611804187627\nEpoch 4/5, Validation Accuracy: 66.27%\nEpoch 5/5, Loss: 0.6289965391817672\nEpoch 5/5, Validation Accuracy: 66.90%\n\nEvaluating on Fold 2/3\nEpoch 1/5, Loss: 0.6257490301659094\nEpoch 1/5, Validation Accuracy: 67.73%\nEpoch 2/5, Loss: 0.6302250356963985\nEpoch 2/5, Validation Accuracy: 68.91%\nEpoch 3/5, Loss: 0.6138986237141308\nEpoch 3/5, Validation Accuracy: 69.81%\nEpoch 4/5, Loss: 0.6061086552577782\nEpoch 4/5, Validation Accuracy: 68.42%\nEpoch 5/5, Loss: 0.6091891309833\nEpoch 5/5, Validation Accuracy: 68.63%\n\nEvaluating on Fold 3/3\nEpoch 1/5, Loss: 0.6087160082482501\nEpoch 1/5, Validation Accuracy: 70.64%\nEpoch 2/5, Loss: 0.6037638961280907\nEpoch 2/5, Validation Accuracy: 69.39%\nEpoch 3/5, Loss: 0.6040307922257903\nEpoch 3/5, Validation Accuracy: 69.04%\nEpoch 4/5, Loss: 0.5936747323412922\nEpoch 4/5, Validation Accuracy: 69.67%\nEpoch 5/5, Loss: 0.596902027973154\nEpoch 5/5, Validation Accuracy: 70.71%\nFold 1 Metrics:\nAccuracy: 0.68, Precision: 0.66, Recall: 0.71, F1-Score: 0.69\nConfusion Matrix:\n[[1205  636]\n [ 510 1258]]\nFold 2 Metrics:\nAccuracy: 0.70, Precision: 0.67, Recall: 0.77, F1-Score: 0.72\nConfusion Matrix:\n[[1117  690]\n [ 410 1392]]\nFold 3 Metrics:\nAccuracy: 0.70, Precision: 0.68, Recall: 0.78, F1-Score: 0.73\nConfusion Matrix:\n[[1075  690]\n [ 400 1443]]\n\nAverage Metrics Across Folds:\nAccuracy: 0.69, Precision: 0.67, Recall: 0.76, F1-Score: 0.71\nConfusion Matrix (sum of all folds):\n[[3397 2016]\n [1320 4093]]\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"def initialize_model(model_name):\n    if model_name == \"inception-v3\":\n        model = models.inception_v3(pretrained=True, aux_logits=True)  \n\n        # Freeze all layers initially\n        for param in model.parameters():\n            param.requires_grad = False\n\n        # Unfreeze the last two convolutional layers (Mixed_7b and Mixed_7c)\n        for param in list(model.Mixed_7b.parameters()) + list(model.Mixed_7c.parameters()):\n            param.requires_grad = True\n\n        # Modify the final fully connected layer for binary classification\n        num_features = model.fc.in_features\n        model.fc = nn.Linear(num_features, 2)  # 2 output classes\n\n        # Print trainable parameters\n        def count_trainable_params(model):\n            return sum(p.numel() for p in model.parameters() if p.requires_grad)\n        print(f\"Total trainable parameters: {count_trainable_params(model):,}\")\n\n        return model\n    else:\n        raise ValueError(\"Model name must be 'inception-v3'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T07:26:26.868609Z","iopub.execute_input":"2024-12-26T07:26:26.868971Z","iopub.status.idle":"2024-12-26T07:26:26.874223Z","shell.execute_reply.started":"2024-12-26T07:26:26.868941Z","shell.execute_reply":"2024-12-26T07:26:26.873145Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# for fine tunning\ndef objective_after(trial, model_name):\n    # Get a suggested learning rate from Optuna\n    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n    \n    # Initialize the model with dropout\n    model = initialize_model(model_name).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n    val_accuracies = []\n    for fold_idx, (train_val_idx, test_idx) in enumerate(kf.split(dataset)):\n        print(f\"Fold {fold_idx + 1}/{num_folds}\")\n        \n        # Create training/validation split\n        train_val_data = Subset(dataset, train_val_idx)\n        test_data = Subset(dataset, test_idx)\n        \n        train_size = int(0.8 * len(train_val_data))\n        val_size = len(train_val_data) - train_size\n        train_data, val_data = torch.utils.data.random_split(\n            train_val_data, [train_size, val_size], generator=torch.Generator().manual_seed(42)\n        )\n        train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n        val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n        \n        # Train the model and get validation accuracy\n        train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs=5)\n        \n        # Evaluate on validation set\n        val_accuracy, _, _, _, _ = calculate_metrics(model, val_loader, device)\n        val_accuracies.append(val_accuracy)\n    \n    # Return the average validation accuracy across all folds as the objective value\n    return np.mean(val_accuracies)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T07:26:35.198607Z","iopub.execute_input":"2024-12-26T07:26:35.198913Z","iopub.status.idle":"2024-12-26T07:26:35.205185Z","shell.execute_reply.started":"2024-12-26T07:26:35.198888Z","shell.execute_reply":"2024-12-26T07:26:35.204246Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def evaluate_test(model_name, best_lr):\n    # Initialize model with the best learning rate\n    model = initialize_model(model_name).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=best_lr)\n    criterion = nn.CrossEntropyLoss()\n\n    fold_metrics = []\n    for fold_idx, (train_val_idx, test_idx) in enumerate(kf.split(dataset)):\n        print(f\"\\nEvaluating on Fold {fold_idx + 1}/{num_folds}\")\n        \n        # Create training/validation split\n        train_val_data = Subset(dataset, train_val_idx)\n        test_data = Subset(dataset, test_idx)\n        \n        train_size = int(0.8 * len(train_val_data))\n        val_size = len(train_val_data) - train_size\n        train_data, val_data = torch.utils.data.random_split(\n            train_val_data, [train_size, val_size], generator=torch.Generator().manual_seed(42)\n        )\n        train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n        val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n        \n        # Train the model\n        train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs=5)\n        \n        # Evaluate on the test set\n        test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n        fold_metrics.append(calculate_metrics(model, test_loader, device))\n    \n    # Print metrics for each fold\n    for fold_idx, metrics in enumerate(fold_metrics):\n        accuracy, precision, recall, f1, conf_matrix = metrics\n        print(f\"Fold {fold_idx + 1} Metrics:\")\n        print(f\"Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}\")\n        print(f\"Confusion Matrix:\\n{conf_matrix}\")\n\n    # Calculate average metrics across folds\n    avg_accuracy = np.mean([metrics[0] for metrics in fold_metrics])\n    avg_precision = np.mean([metrics[1] for metrics in fold_metrics])\n    avg_recall = np.mean([metrics[2] for metrics in fold_metrics])\n    avg_f1 = np.mean([metrics[3] for metrics in fold_metrics])\n    total_conf_matrix = np.sum([metrics[4] for metrics in fold_metrics], axis=0)\n\n    print(\"\\nAverage Metrics Across Folds:\")\n    print(f\"Accuracy: {avg_accuracy:.2f}, Precision: {avg_precision:.2f}, Recall: {avg_recall:.2f}, F1-Score: {avg_f1:.2f}\")\n    print(f\"Confusion Matrix (sum of all folds):\\n{total_conf_matrix}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T07:26:40.100814Z","iopub.execute_input":"2024-12-26T07:26:40.101163Z","iopub.status.idle":"2024-12-26T07:26:40.109714Z","shell.execute_reply.started":"2024-12-26T07:26:40.101136Z","shell.execute_reply":"2024-12-26T07:26:40.108921Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"for model_name in [\"inception-v3\"]:\n    print(f\"\\nOptimizing for {model_name.upper()}...\")\n    study = optuna.create_study(direction='maximize')\n    study.optimize(lambda trial: objective_after(trial, model_name), n_trials=5)  # You can increase the number of trials if needed\n\n    # Best learning rate found for the model\n    best_lr = study.best_params['lr']\n    print(f\"Best Learning Rate for {model_name.upper()}: {best_lr}\")\n\n    # Evaluate on test sets for each fold\n    evaluate_test(model_name, best_lr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T07:26:45.093520Z","iopub.execute_input":"2024-12-26T07:26:45.093827Z","iopub.status.idle":"2024-12-26T12:02:52.665472Z","shell.execute_reply.started":"2024-12-26T07:26:45.093801Z","shell.execute_reply":"2024-12-26T12:02:52.664722Z"}},"outputs":[{"name":"stderr","text":"[I 2024-12-26 07:26:45,095] A new study created in memory with name: no-name-359ad20d-9a95-4576-a621-b7f5709cb1c2\n","output_type":"stream"},{"name":"stdout","text":"\nOptimizing for INCEPTION-V3...\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-10-fd9538108a4f>:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n100%|██████████| 104M/104M [00:00<00:00, 166MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"Total trainable parameters: 11,125,506\nFold 1/3\nEpoch 1/5, Loss: 0.5638978836944749\nEpoch 1/5, Validation Accuracy: 73.27%\nEpoch 2/5, Loss: 0.4733071730611074\nEpoch 2/5, Validation Accuracy: 77.15%\nEpoch 3/5, Loss: 0.43890419017873417\nEpoch 3/5, Validation Accuracy: 79.78%\nEpoch 4/5, Loss: 0.40531234788960513\nEpoch 4/5, Validation Accuracy: 77.77%\nEpoch 5/5, Loss: 0.4050169918089282\nEpoch 5/5, Validation Accuracy: 79.78%\nFold 2/3\nEpoch 1/5, Loss: 0.4193113028015221\nEpoch 1/5, Validation Accuracy: 83.45%\nEpoch 2/5, Loss: 0.40484101485810886\nEpoch 2/5, Validation Accuracy: 82.76%\nEpoch 3/5, Loss: 0.37655094694037466\nEpoch 3/5, Validation Accuracy: 83.24%\nEpoch 4/5, Loss: 0.37620787338986583\nEpoch 4/5, Validation Accuracy: 84.49%\nEpoch 5/5, Loss: 0.36646624576320963\nEpoch 5/5, Validation Accuracy: 82.20%\nFold 3/3\nEpoch 1/5, Loss: 0.3744753759704242\nEpoch 1/5, Validation Accuracy: 84.63%\nEpoch 2/5, Loss: 0.3587072285005401\nEpoch 2/5, Validation Accuracy: 83.59%\nEpoch 3/5, Loss: 0.35018036452775503\nEpoch 3/5, Validation Accuracy: 84.97%\nEpoch 4/5, Loss: 0.3454007400992167\nEpoch 4/5, Validation Accuracy: 84.21%\nEpoch 5/5, Loss: 0.3420929960618362\nEpoch 5/5, Validation Accuracy: 85.11%\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-26 08:13:04,611] Trial 0 finished with value: 0.8277931671283473 and parameters: {'lr': 0.004118722506403129}. Best is trial 0 with value: 0.8277931671283473.\n<ipython-input-10-fd9538108a4f>:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Total trainable parameters: 11,125,506\nFold 1/3\nEpoch 1/5, Loss: 0.5477964446689543\nEpoch 1/5, Validation Accuracy: 78.32%\nEpoch 2/5, Loss: 0.4703672540615935\nEpoch 2/5, Validation Accuracy: 77.63%\nEpoch 3/5, Loss: 0.43870944665611117\nEpoch 3/5, Validation Accuracy: 80.47%\nEpoch 4/5, Loss: 0.41285893839696497\nEpoch 4/5, Validation Accuracy: 79.43%\nEpoch 5/5, Loss: 0.4110098449894078\nEpoch 5/5, Validation Accuracy: 75.90%\nFold 2/3\nEpoch 1/5, Loss: 0.42088866637227285\nEpoch 1/5, Validation Accuracy: 81.51%\nEpoch 2/5, Loss: 0.3963907667286488\nEpoch 2/5, Validation Accuracy: 82.89%\nEpoch 3/5, Loss: 0.3958542943824062\nEpoch 3/5, Validation Accuracy: 81.99%\nEpoch 4/5, Loss: 0.38853428574556803\nEpoch 4/5, Validation Accuracy: 83.59%\nEpoch 5/5, Loss: 0.3696455528228981\nEpoch 5/5, Validation Accuracy: 84.42%\nFold 3/3\nEpoch 1/5, Loss: 0.3804966535686788\nEpoch 1/5, Validation Accuracy: 85.87%\nEpoch 2/5, Loss: 0.37423470624573324\nEpoch 2/5, Validation Accuracy: 84.90%\nEpoch 3/5, Loss: 0.35146289339381687\nEpoch 3/5, Validation Accuracy: 83.38%\nEpoch 4/5, Loss: 0.33511270072249416\nEpoch 4/5, Validation Accuracy: 83.17%\nEpoch 5/5, Loss: 0.3342884693356509\nEpoch 5/5, Validation Accuracy: 84.21%\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-26 08:59:05,384] Trial 1 finished with value: 0.8155586334256695 and parameters: {'lr': 0.003752533911020552}. Best is trial 0 with value: 0.8277931671283473.\n<ipython-input-10-fd9538108a4f>:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Total trainable parameters: 11,125,506\nFold 1/3\nEpoch 1/5, Loss: 0.5403800432194662\nEpoch 1/5, Validation Accuracy: 74.03%\nEpoch 2/5, Loss: 0.4407261014807949\nEpoch 2/5, Validation Accuracy: 79.22%\nEpoch 3/5, Loss: 0.40828904830619117\nEpoch 3/5, Validation Accuracy: 79.57%\nEpoch 4/5, Loss: 0.3760490473462732\nEpoch 4/5, Validation Accuracy: 78.53%\nEpoch 5/5, Loss: 0.3635088159234484\nEpoch 5/5, Validation Accuracy: 80.06%\nFold 2/3\nEpoch 1/5, Loss: 0.3876498246884478\nEpoch 1/5, Validation Accuracy: 84.63%\nEpoch 2/5, Loss: 0.37032375892222913\nEpoch 2/5, Validation Accuracy: 84.35%\nEpoch 3/5, Loss: 0.34786397210471537\nEpoch 3/5, Validation Accuracy: 83.59%\nEpoch 4/5, Loss: 0.32761200229272\nEpoch 4/5, Validation Accuracy: 82.89%\nEpoch 5/5, Loss: 0.32627432945683515\nEpoch 5/5, Validation Accuracy: 84.76%\nFold 3/3\nEpoch 1/5, Loss: 0.33839414717413446\nEpoch 1/5, Validation Accuracy: 85.11%\nEpoch 2/5, Loss: 0.32601217588008435\nEpoch 2/5, Validation Accuracy: 86.36%\nEpoch 3/5, Loss: 0.3061660928703145\nEpoch 3/5, Validation Accuracy: 85.53%\nEpoch 4/5, Loss: 0.29295815580951573\nEpoch 4/5, Validation Accuracy: 84.76%\nEpoch 5/5, Loss: 0.27459735818495407\nEpoch 5/5, Validation Accuracy: 84.14%\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-26 09:44:07,275] Trial 2 finished with value: 0.8287165281625116 and parameters: {'lr': 0.000138700158394391}. Best is trial 2 with value: 0.8287165281625116.\n<ipython-input-10-fd9538108a4f>:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Total trainable parameters: 11,125,506\nFold 1/3\nEpoch 1/5, Loss: 0.5287911766800433\nEpoch 1/5, Validation Accuracy: 72.85%\nEpoch 2/5, Loss: 0.44426243649332564\nEpoch 2/5, Validation Accuracy: 75.28%\nEpoch 3/5, Loss: 0.4033638508115684\nEpoch 3/5, Validation Accuracy: 80.06%\nEpoch 4/5, Loss: 0.37710862355666924\nEpoch 4/5, Validation Accuracy: 79.29%\nEpoch 5/5, Loss: 0.3557982919789151\nEpoch 5/5, Validation Accuracy: 82.06%\nFold 2/3\nEpoch 1/5, Loss: 0.3875230983805261\nEpoch 1/5, Validation Accuracy: 83.38%\nEpoch 2/5, Loss: 0.3634129394648483\nEpoch 2/5, Validation Accuracy: 84.90%\nEpoch 3/5, Loss: 0.3433737968871607\nEpoch 3/5, Validation Accuracy: 82.96%\nEpoch 4/5, Loss: 0.33424263632758544\nEpoch 4/5, Validation Accuracy: 84.35%\nEpoch 5/5, Loss: 0.31496187427096606\nEpoch 5/5, Validation Accuracy: 83.59%\nFold 3/3\nEpoch 1/5, Loss: 0.34145854899237826\nEpoch 1/5, Validation Accuracy: 85.73%\nEpoch 2/5, Loss: 0.3263457348333538\nEpoch 2/5, Validation Accuracy: 85.39%\nEpoch 3/5, Loss: 0.30687269947146845\nEpoch 3/5, Validation Accuracy: 86.22%\nEpoch 4/5, Loss: 0.2869988755214939\nEpoch 4/5, Validation Accuracy: 85.46%\nEpoch 5/5, Loss: 0.2734402253894516\nEpoch 5/5, Validation Accuracy: 85.11%\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-26 10:29:43,586] Trial 3 finished with value: 0.8252539242843951 and parameters: {'lr': 0.00017009810435342036}. Best is trial 2 with value: 0.8287165281625116.\n<ipython-input-10-fd9538108a4f>:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Total trainable parameters: 11,125,506\nFold 1/3\nEpoch 1/5, Loss: 0.5446720842827749\nEpoch 1/5, Validation Accuracy: 74.52%\nEpoch 2/5, Loss: 0.4553913488901781\nEpoch 2/5, Validation Accuracy: 79.16%\nEpoch 3/5, Loss: 0.43898604653816853\nEpoch 3/5, Validation Accuracy: 80.06%\nEpoch 4/5, Loss: 0.4238336817335687\nEpoch 4/5, Validation Accuracy: 78.95%\nEpoch 5/5, Loss: 0.395289893390724\nEpoch 5/5, Validation Accuracy: 79.50%\nFold 2/3\nEpoch 1/5, Loss: 0.41691509086782763\nEpoch 1/5, Validation Accuracy: 83.73%\nEpoch 2/5, Loss: 0.4008506108384106\nEpoch 2/5, Validation Accuracy: 83.66%\nEpoch 3/5, Loss: 0.4060938256221582\nEpoch 3/5, Validation Accuracy: 82.89%\nEpoch 4/5, Loss: 0.3742943604512768\nEpoch 4/5, Validation Accuracy: 82.96%\nEpoch 5/5, Loss: 0.3718218248522743\nEpoch 5/5, Validation Accuracy: 82.76%\nFold 3/3\nEpoch 1/5, Loss: 0.3784575177490382\nEpoch 1/5, Validation Accuracy: 83.38%\nEpoch 2/5, Loss: 0.368230149849673\nEpoch 2/5, Validation Accuracy: 83.73%\nEpoch 3/5, Loss: 0.35088177683933003\nEpoch 3/5, Validation Accuracy: 84.63%\nEpoch 4/5, Loss: 0.3511695212257501\nEpoch 4/5, Validation Accuracy: 81.44%\nEpoch 5/5, Loss: 0.3268813270231637\nEpoch 5/5, Validation Accuracy: 79.02%\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-12-26 11:14:45,923] Trial 4 finished with value: 0.814173591874423 and parameters: {'lr': 0.0042578548751414326}. Best is trial 2 with value: 0.8287165281625116.\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Best Learning Rate for INCEPTION-V3: 0.000138700158394391\nTotal trainable parameters: 11,125,506\n\nEvaluating on Fold 1/3\nEpoch 1/5, Loss: 0.5376401125396812\nEpoch 1/5, Validation Accuracy: 76.39%\nEpoch 2/5, Loss: 0.44337052600818444\nEpoch 2/5, Validation Accuracy: 77.84%\nEpoch 3/5, Loss: 0.40468366024243896\nEpoch 3/5, Validation Accuracy: 78.46%\nEpoch 4/5, Loss: 0.3850684403055939\nEpoch 4/5, Validation Accuracy: 78.67%\nEpoch 5/5, Loss: 0.354561274881521\nEpoch 5/5, Validation Accuracy: 81.09%\n\nEvaluating on Fold 2/3\nEpoch 1/5, Loss: 0.3820158474517791\nEpoch 1/5, Validation Accuracy: 84.76%\nEpoch 2/5, Loss: 0.36270403071661683\nEpoch 2/5, Validation Accuracy: 83.24%\nEpoch 3/5, Loss: 0.34414899662054704\nEpoch 3/5, Validation Accuracy: 83.31%\nEpoch 4/5, Loss: 0.32476432983717207\nEpoch 4/5, Validation Accuracy: 85.04%\nEpoch 5/5, Loss: 0.3107558508604271\nEpoch 5/5, Validation Accuracy: 83.59%\n\nEvaluating on Fold 3/3\nEpoch 1/5, Loss: 0.34957296393029597\nEpoch 1/5, Validation Accuracy: 85.39%\nEpoch 2/5, Loss: 0.3195489608780455\nEpoch 2/5, Validation Accuracy: 85.60%\nEpoch 3/5, Loss: 0.314655560403239\nEpoch 3/5, Validation Accuracy: 85.04%\nEpoch 4/5, Loss: 0.30449836725852764\nEpoch 4/5, Validation Accuracy: 85.25%\nEpoch 5/5, Loss: 0.2937597559137239\nEpoch 5/5, Validation Accuracy: 86.63%\nFold 1 Metrics:\nAccuracy: 0.81, Precision: 0.81, Recall: 0.81, F1-Score: 0.81\nConfusion Matrix:\n[[1502  339]\n [ 336 1432]]\nFold 2 Metrics:\nAccuracy: 0.85, Precision: 0.83, Recall: 0.88, F1-Score: 0.85\nConfusion Matrix:\n[[1479  328]\n [ 220 1582]]\nFold 3 Metrics:\nAccuracy: 0.87, Precision: 0.87, Recall: 0.88, F1-Score: 0.87\nConfusion Matrix:\n[[1529  236]\n [ 228 1615]]\n\nAverage Metrics Across Folds:\nAccuracy: 0.84, Precision: 0.84, Recall: 0.85, F1-Score: 0.85\nConfusion Matrix (sum of all folds):\n[[4510  903]\n [ 784 4629]]\n","output_type":"stream"}],"execution_count":12}]}